{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3e81eac6",
        "outputId": "3fece6f4-d2cc-4ff4-9e25-cb3364643170"
      },
      "source": [
        "%pip install langchain_huggingface langchain_core python-dotenv"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_huggingface\n",
            "  Downloading langchain_huggingface-1.0.0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: langchain_core in /usr/local/lib/python3.12/dist-packages (0.3.79)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.1.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.33.4 in /usr/local/lib/python3.12/dist-packages (from langchain_huggingface) (0.35.3)\n",
            "Collecting langchain_core\n",
            "  Downloading langchain_core-1.0.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: tokenizers<1.0.0,>=0.19.1 in /usr/local/lib/python3.12/dist-packages (from langchain_huggingface) (0.22.1)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (0.4.35)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (25.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (2.11.10)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (1.1.10)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain_core) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain_core) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain_core) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain_core) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain_core) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain_core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain_core) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain_core) (0.4.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain_core) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain_core) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain_core) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain_core) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain_core) (0.16.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain_core) (1.3.1)\n",
            "Downloading langchain_huggingface-1.0.0-py3-none-any.whl (27 kB)\n",
            "Downloading langchain_core-1.0.0-py3-none-any.whl (467 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m467.2/467.2 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain_core, langchain_huggingface\n",
            "  Attempting uninstall: langchain_core\n",
            "    Found existing installation: langchain-core 0.3.79\n",
            "    Uninstalling langchain-core-0.3.79:\n",
            "      Successfully uninstalled langchain-core-0.3.79\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain 0.3.27 requires langchain-core<1.0.0,>=0.3.72, but you have langchain-core 1.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed langchain_core-1.0.0 langchain_huggingface-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkPT0j2-y2kE",
        "outputId": "446d3777-a4e8-4000-e446-dc0539d86864"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's a 5-line summary of the text:\n",
            "\n",
            "- Black holes are incredibly dense areas in space with such strong gravity that even light cannot escape, making them \"invisible\".\n",
            "-  They form when massive stars collapse, creating a singularity of infinite density surrounded by an \"event horizon\".\n",
            "-  Black holes were theorized by Einstein and are divided into stellar-mass, supermassive, and intermediate-mass types based on their mass and influence on their surroundings.\n",
            "- Although invisible, their effects are observable through things like gravitational lensing and superheated disks.\n",
            "- Research into black holes focuses on understanding their formation, behavior, and potential connection to the fabric of spacetime. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
        "from dotenv import load_dotenv\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"google/gemma-2-2b-it\",\n",
        "    task=\"text-generation\"\n",
        ")\n",
        "\n",
        "model = ChatHuggingFace(llm=llm)\n",
        "\n",
        "# 1st prompt -> detailed report\n",
        "template1 = PromptTemplate(\n",
        "    template='Write a detailed report on {topic}',\n",
        "    input_variables=['topic']\n",
        ")\n",
        "\n",
        "# 2nd prompt -> summary\n",
        "template2 = PromptTemplate(\n",
        "    template='Write a 5 line summary on the following text. /n {text}',\n",
        "    input_variables=['text']\n",
        ")\n",
        "\n",
        "prompt1 = template1.invoke({'topic':'black hole'})\n",
        "\n",
        "result = model.invoke(prompt1)\n",
        "\n",
        "prompt2 = template2.invoke({'text':result.content})\n",
        "\n",
        "result1 = model.invoke(prompt2)\n",
        "\n",
        "print(result1.content)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "188aa71d",
        "outputId": "3a03f50d-160b-435d-d173-9582944a3e8b"
      },
      "source": [
        "%pip install langchain_openai"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-1.0.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (1.0.0)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (1.109.1)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (0.12.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain_openai) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain_openai) (0.4.35)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain_openai) (25.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain_openai) (2.11.10)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain_openai) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain_openai) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain_openai) (4.15.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (0.11.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain_openai) (2.32.4)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<3.0.0,>=1.109.1->langchain_openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain_openai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain_openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain_openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain_openai) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain_openai) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain_openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain_openai) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain_openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain_openai) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain_openai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain_openai) (2.5.0)\n",
            "Downloading langchain_openai-1.0.0-py3-none-any.whl (80 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.5/80.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain_openai\n",
            "Successfully installed langchain_openai-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from dotenv import load_dotenv\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "import os\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Define the model\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"google/gemma-2-2b-it\",\n",
        "    task=\"text-generation\"\n",
        ")\n",
        "# 1st prompt -> detailed report\n",
        "template1 = PromptTemplate(\n",
        "    template='Write a detailed report on {topic}',\n",
        "    input_variables=['topic']\n",
        ")\n",
        "\n",
        "# 2nd prompt -> summary\n",
        "template2 = PromptTemplate(\n",
        "    template='Write a 5 line summary on the following text. /n {text}',\n",
        "    input_variables=['text']\n",
        ")\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "chain = template1 | model | parser | template2 | model | parser\n",
        "\n",
        "result = chain.invoke({'topic':'black hole'})\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6TweCMxmzVzu",
        "outputId": "5b590ecc-35b6-4a56-df5f-f4b8b259f0bc"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Black holes are regions of spacetime with such extreme gravity that nothing, not even light, can escape. Formed from the collapse of massive stars, black holes possess a singularity, an infinitely dense point, and an event horizon beyond which escape is impossible.  Supermassive black holes at galaxy centers play a crucial role in galaxy formation and evolution, while collisions of black holes produce powerful bursts of energy. Open questions surrounding black holes include the information paradox and the quantum nature of gravity. Research into black holes continues to unveil ever more mysteries while pushing the boundaries of our understanding of the universe. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
        "from dotenv import load_dotenv\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Define the model\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"google/gemma-2-2b-it\",\n",
        "    task=\"text-generation\"\n",
        ")\n",
        "\n",
        "model = ChatHuggingFace(llm=llm)\n",
        "\n",
        "parser = JsonOutputParser()\n",
        "\n",
        "template = PromptTemplate(\n",
        "    template='Give me 5 facts about {topic} in JSON format {format_instruction}',\n",
        "    input_variables=['topic'],\n",
        "    partial_variables={'format_instruction': parser.get_format_instructions()}\n",
        ")\n",
        "\n",
        "chain = template | model | parser\n",
        "\n",
        "result = chain.invoke({'topic':'black hole'})\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3U4ytgkc-PIo",
        "outputId": "52004033-cb56-4eb0-db48-0d5ccd2aba69"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'facts': ['Black holes are regions of spacetime where gravity is so strong that nothing, not even light, can escape.', 'They are formed when massive stars collapse at the end of their lives.', 'Black holes come in different sizes, with the most massive ones called supermassive black holes, found at the centers of galaxies.', \"Despite their name, black holes aren't actually holes, but rather incredibly dense objects.\", 'We can detect black holes indirectly through their gravitational influence on nearby stars and gas clouds.']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
        "from dotenv import load_dotenv\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"google/gemma-2-2b-it\", # Changed model to a chat-optimized one\n",
        "    task=\"text-generation\"\n",
        ")\n",
        "model = ChatHuggingFace(llm=llm)\n",
        "\n",
        "parser = JsonOutputParser()\n",
        "\n",
        "template = PromptTemplate(\n",
        "    #template='Give me the name, age, and city of a fictional person in JSON format.', # Explicitly ask for JSON\n",
        "    template = 'Give me 5 facts about {topic} in JSON format {format_instruction}',\n",
        "    input_variables=['topic'],\n",
        "    partial_variables= {'format_instruction':parser.get_format_instructions()}\n",
        ")\n",
        "\n",
        "chain = template | model | parser # The parser will handle the JSON string output\n",
        "\n",
        "# prompt = template.format() # Removed this line\n",
        "\n",
        "#print(prompt)\n",
        "\n",
        "#result = chain.invoke({}) # Invoke with an empty dictionary as there are no input variables\n",
        "result = chain.invoke({'topic':'Indian Education'})\n",
        "# final_result = parser.parse(result.content) # Remove redundant parser call\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrXn-WRL_sYq",
        "outputId": "b28031ab-4f74-4c8e-dc66-eb3c8b0c49da"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'fact': 'India boasts a diverse, long-standing education system.', 'detail': 'It combines ancient traditional learning with modern, global influences.  Vast variations exist across states and regions.'}, {'fact': 'Literacy rates have seen significant growth.', 'detail': 'Efforts throughout the nation have increased literacy, particularly among rural populations, but disparities between genders and socioeconomic backgrounds persist.'}, {'fact': 'English is a prevalent language of instruction, but language diversity is found throughout classroom settings.', 'detail': 'While English is widely taught at the higher levels, regional languages are often initially the starting point for child education'}, {'fact': 'India has a strong vocational and technical education sector.', 'detail': \"At all levels, there's a focus on vocational skills development, especially in areas of STEM research, agriculture, and trade.\"}, {'fact': 'Substantial investment in public education is crucial for national progress.', 'detail': 'Despite challenges, the Indian government continuously refines and expands its public school system, with ongoing efforts to improve teacher quality and infrastructure.'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
        "from dotenv import load_dotenv\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Define the model\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"google/gemma-2-2b-it\",\n",
        "    task=\"text-generation\"\n",
        ")\n",
        "\n",
        "model = ChatHuggingFace(llm=llm)\n",
        "\n",
        "schema = [\n",
        "    ResponseSchema(name='fact_1', description='Fact 1 about the topic'),\n",
        "    ResponseSchema(name='fact_2', description='Fact 2 about the topic'),\n",
        "    ResponseSchema(name='fact_3', description='Fact 3 about the topic'),\n",
        "]\n",
        "\n",
        "parser = StructuredOutputParser.from_response_schemas(schema)\n",
        "\n",
        "template = PromptTemplate(\n",
        "    template='Give 3 fact about {topic} \\n {format_instruction}',\n",
        "    input_variables=['topic'],\n",
        "    partial_variables={'format_instruction':parser.get_format_instructions()}\n",
        ")\n",
        "\n",
        "chain = template | model | parser\n",
        "\n",
        "result = chain.invoke({'topic':'Pradhanmantri AWAS Yogna'})\n",
        "\n",
        "print(result)\n",
        "\n",
        "result2 = chain.invoke({'topic':'Indian Education after Graduation'})\n",
        "\n",
        "print(result2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7w0jmWrEy7h",
        "outputId": "7ebeb537-3737-40cc-faed-71d9ab150223"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'fact_1': 'Pradhanmantri Awas Yojana (PMAY) is a flagship program of the Government of India launched in 2015.', 'fact_2': 'The scheme aims to provide affordable housing to all rural and urban residents across India, focusing on economically weaker sections and marginalized groups.', 'fact_3': 'PMAY provides funding for both the construction of new houses and also for renovating or improving existing ones.'}\n",
            "{'fact_1': 'In India, there are a wide range of opportunities available after graduation, including government jobs, public sector organizations, NGOs, and private sector companies.', 'fact_2': \"There's a significant focus on core skills development, practical knowledge and hands-on experience through internships and projects during the post degree work experience.\", 'fact_3': 'The perceived value of higher education varies in different parts of the country, making career choices a thoughtful process based on individual priorities and the job market in various regions.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Same with chain"
      ],
      "metadata": {
        "id": "20J5-DAtFilT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
        "from dotenv import load_dotenv\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Define the model\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"google/gemma-2-2b-it\",\n",
        "    task=\"text-generation\"\n",
        ")\n",
        "\n",
        "model = ChatHuggingFace(llm=llm)\n",
        "\n",
        "schema = [\n",
        "    ResponseSchema(name='fact_1', description='Fact 1 about the topic'),\n",
        "    ResponseSchema(name='fact_2', description='Fact 2 about the topic'),\n",
        "    ResponseSchema(name='fact_3', description='Fact 3 about the topic'),\n",
        "]\n",
        "\n",
        "parser = StructuredOutputParser.from_response_schemas(schema)\n",
        "\n",
        "template = PromptTemplate(\n",
        "    template='Give 3 fact about {topic} \\n {format_instruction}',\n",
        "    input_variables=['topic'],\n",
        "    partial_variables={'format_instruction':parser.get_format_instructions()}\n",
        ")\n",
        "\n",
        "prompt = template.format(topic='Pradhanmantri AWAS Yogna')\n",
        "\n",
        "result = model.invoke(prompt)\n",
        "\n",
        "result_final = parser.parse(result.content)\n",
        "\n",
        "print(result_final)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_7Upt1lFo6A",
        "outputId": "856dd235-636a-45c6-da36-c2bb149373f7"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'fact_1': 'The Pradhan Mantri Awas Yojana (PMAY) is a flagship program aimed at providing affordable housing to all economically weaker sections of the society by 2022.', 'fact_2': 'It aims to achieve this goal through both financial assistance and support for land acquisition, construction, repair, and registration.', 'fact_3': 'The program is a major economic and social initiative of the Bharatiya Janata Party (BJP) government.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pydantic Output Parser"
      ],
      "metadata": {
        "id": "EwmIbCfCGsea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
        "from dotenv import load_dotenv\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import PydanticOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Define the model\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"google/gemma-2-2b-it\",\n",
        "    task=\"text-generation\"\n",
        ")\n",
        "\n",
        "model = ChatHuggingFace(llm=llm)\n",
        "\n",
        "class Person(BaseModel):\n",
        "\n",
        "    name: str = Field(description='Name of the person')\n",
        "    age: int = Field(gt=18, description='Age of the person')\n",
        "    city: str = Field(description='Name of the city the person belongs to')\n",
        "\n",
        "parser = PydanticOutputParser(pydantic_object=Person)\n",
        "\n",
        "template = PromptTemplate(\n",
        "    template='Generate the name, age and city of a fictional {place} person \\n {format_instruction}',\n",
        "    input_variables=['place'],\n",
        "    partial_variables={'format_instruction':parser.get_format_instructions()}\n",
        ")\n",
        "\n",
        "prompt = template.invoke({'place':'sri lankan'})\n",
        "\n",
        "result = model.invoke(prompt)\n",
        "\n",
        "final_result = parser.parse(result.content)\n",
        "final_result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31LhEEQ9Gvfv",
        "outputId": "34864b16-e438-4243-a63f-a96e61406afe"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Person(name='Ranil Silva', age=32, city='Colombo')"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
        "from dotenv import load_dotenv\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import PydanticOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Define the model\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"google/gemma-2-2b-it\",\n",
        "    task=\"text-generation\"\n",
        ")\n",
        "\n",
        "model = ChatHuggingFace(llm=llm)\n",
        "\n",
        "class Person(BaseModel):\n",
        "\n",
        "    name: str = Field(description='Name of the person')\n",
        "    age: int = Field(gt=18, description='Age of the person')\n",
        "    city: str = Field(description='Name of the city the person belongs to')\n",
        "\n",
        "parser = PydanticOutputParser(pydantic_object=Person)\n",
        "\n",
        "template = PromptTemplate(\n",
        "    template='Generate the name, age and city of a fictional {place} person \\n {format_instruction}',\n",
        "    input_variables=['place'],\n",
        "    partial_variables={'format_instruction':parser.get_format_instructions()}\n",
        ")\n",
        "chain = template | model | parser\n",
        "\n",
        "final_result = chain.invoke({'place':'Australia'})\n",
        "\n",
        "print(final_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67jb9jdkHJCg",
        "outputId": "14844787-faac-46b7-8da8-0acbb582b674"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name='Zoe Reynolds' age=28 city='Melbourne'\n"
          ]
        }
      ]
    }
  ]
}