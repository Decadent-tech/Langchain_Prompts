{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# sample chain"
      ],
      "metadata": {
        "id": "7X2ntwVT2dzz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# 🧹 Clean Install (isolated compatible LangChain setup)\n",
        "# =====================================================\n",
        "\n",
        "# 1️⃣ Uninstall all conflicting versions\n",
        "!pip uninstall -y langchain langchain-core langchain-openai langchain-community langchain-classic langgraph-prebuilt pydantic pydantic-core\n",
        "\n",
        "# 2️⃣ Install the correct compatible trio + dotenv\n",
        "!pip install -qU langchain==0.3.7 langchain-core==0.3.15 langchain-openai==0.2.6 python-dotenv\n",
        "\n",
        "# 3️⃣ Downgrade pydantic slightly to a stable build\n",
        "!pip install -qU \"pydantic==2.8.2\" \"pydantic-core==2.20.1\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eO6dlp9wi7sh",
        "outputId": "2a0cbc76-9949-4e8c-e247-3be9103b552f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: langchain 1.0.2\n",
            "Uninstalling langchain-1.0.2:\n",
            "  Successfully uninstalled langchain-1.0.2\n",
            "Found existing installation: langchain-core 1.0.0\n",
            "Uninstalling langchain-core-1.0.0:\n",
            "  Successfully uninstalled langchain-core-1.0.0\n",
            "\u001b[33mWARNING: Skipping langchain-openai as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping langchain-community as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping langchain-classic as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: langgraph-prebuilt 1.0.1\n",
            "Uninstalling langgraph-prebuilt-1.0.1:\n",
            "  Successfully uninstalled langgraph-prebuilt-1.0.1\n",
            "Found existing installation: pydantic 2.12.3\n",
            "Uninstalling pydantic-2.12.3:\n",
            "  Successfully uninstalled pydantic-2.12.3\n",
            "Found existing installation: pydantic_core 2.41.4\n",
            "Uninstalling pydantic_core-2.41.4:\n",
            "  Successfully uninstalled pydantic_core-2.41.4\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m948.6/948.6 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langgraph 1.0.1 requires langgraph-prebuilt<1.1.0,>=1.0.0, which is not installed.\n",
            "google-adk 1.16.0 requires tenacity<9.0.0,>=8.0.0, but you have tenacity 9.1.2 which is incompatible.\n",
            "gradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.3 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langgraph 1.0.1 requires langgraph-prebuilt<1.1.0,>=1.0.0, which is not installed.\n",
            "albumentations 2.0.8 requires pydantic>=2.9.2, but you have pydantic 2.8.2 which is incompatible.\n",
            "mcp 1.18.0 requires pydantic<3.0.0,>=2.11.0, but you have pydantic 2.8.2 which is incompatible.\n",
            "google-adk 1.16.0 requires tenacity<9.0.0,>=8.0.0, but you have tenacity 9.1.2 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# 🔹 STEP 2: Import Libraries\n",
        "# =====================================================\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# =====================================================\n",
        "# 🔹 STEP 3: Load OpenAI API Key\n",
        "# =====================================================\n",
        "\n",
        "# Option 1️⃣: If you have a .env file in your Google Drive or Colab environment\n",
        "# Uncomment this line if .env exists\n",
        "# load_dotenv()\n",
        "\n",
        "# Option 2️⃣: Directly set your key here (recommended for Colab demo)\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"  # <-- replace with your real key\n",
        "\n",
        "# =====================================================\n",
        "# 🔹 STEP 4: Build the LangChain Pipeline\n",
        "# =====================================================\n",
        "prompt = PromptTemplate(\n",
        "    template='Generate 5 interesting facts about {topic}.',\n",
        "    input_variables=['topic']\n",
        ")\n",
        "from langchain_openai import ChatOpenAI\n",
        "ChatOpenAI.model_rebuild()  # 👈 critical patch line\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\")  # safe small model for testing\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "chain = prompt | model | parser\n",
        "\n",
        "# =====================================================\n",
        "# 🔹 STEP 5: Invoke the Chain\n",
        "# =====================================================\n",
        "result = chain.invoke({'topic': 'cricket'})\n",
        "print(\"✅ Generated Output:\\n\")\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 948
        },
        "id": "3BpDYT6libO8",
        "outputId": "91b7b65b-f90d-417c-c2ea-c8576d26e29c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Generated Output:\n",
            "\n",
            "Sure! Here are five interesting facts about cricket:\n",
            "\n",
            "1. **Origin and History**: Cricket dates back to the 16th century in England. The earliest known reference to the game was in a 1597 court case where the sport was mentioned, indicating its popularity even in that era. The sport evolved over the centuries, with the first known rules being codified in 1744.\n",
            "\n",
            "2. **World's Longest Match**: The longest recorded cricket match took place in 1939 between England and South Africa. It lasted for an astonishing 9 days (from March 3 to March 14) at Durban. The match was declared a draw after South Africa batted for 1,098 runs in their second innings.\n",
            "\n",
            "3. **The Ashes**: The Ashes is one of cricket's most famous rivalries, contested between England and Australia. The term originated in 1882 when Australia defeated England for the first time on English soil. A mock obituary published in a British newspaper stated that English cricket had died, and \"the body will be cremated, and the ashes taken to Australia.\" This led to the creation of the Ashes trophy.\n",
            "\n",
            "4. **Cricket World Cup**: The ICC Cricket World Cup, first held in 1975, is one of the most prestigious tournaments in the sport. The 2019 World Cup was particularly notable as it featured the first-ever Super Over in a final match. England won the tournament against New Zealand based on the boundary count after both teams ended up with the same score in the Super Over.\n",
            "\n",
            "5. **The Unique Dismissal**: One of the most unique ways to get out in cricket is \"obstructing the field.\" A batsman can be given out if they deliberately obstruct a fielder with their bat or body while the ball is in play. This rare dismissal is often seen as a sign of unsportsmanlike conduct and is not common in matches.\n",
            "\n",
            "These facts highlight the rich history and unique aspects of cricket, making it a fascinating sport enjoyed by millions around the world!\n",
            "\n",
            "🧩 Chain Structure:\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "Install grandalf to draw graphs: `pip install grandalf`.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/graph_ascii.py\u001b[0m in \u001b[0;36m_build_sugiyama_layout\u001b[0;34m(vertices, edges)\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mgrandalf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraphs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEdge\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVertex\u001b[0m  \u001b[0;31m# type: ignore[import]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mgrandalf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayouts\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSugiyamaLayout\u001b[0m  \u001b[0;31m# type: ignore[import]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'grandalf'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4003157836.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# =====================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n🧩 Chain Structure:\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/graph.py\u001b[0m in \u001b[0;36mprint_ascii\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    485\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprint_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[0;34m\"\"\"Print the graph as an ASCII art string.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# noqa: T201\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/graph.py\u001b[0m in \u001b[0;36mdraw_ascii\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunnables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_ascii\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdraw_ascii\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m         return draw_ascii(\n\u001b[0m\u001b[1;32m    481\u001b[0m             \u001b[0;34m{\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medges\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/graph_ascii.py\u001b[0m in \u001b[0;36mdraw_ascii\u001b[0;34m(vertices, edges)\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0mylist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m     \u001b[0msug\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_build_sugiyama_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvertices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medges\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mvertex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msug\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msV\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/graph_ascii.py\u001b[0m in \u001b[0;36m_build_sugiyama_layout\u001b[0;34m(vertices, edges)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Install grandalf to draw graphs: `pip install grandalf`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: Install grandalf to draw graphs: `pip install grandalf`.",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install grandalf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8F50Yk_olJBJ",
        "outputId": "15a7a562-915a-4291-fae7-ae68033c9c03"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting grandalf\n",
            "  Downloading grandalf-0.8-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from grandalf) (3.2.5)\n",
            "Downloading grandalf-0.8-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: grandalf\n",
            "Successfully installed grandalf-0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# 🔹 STEP 6: Visualize Chain Graph\n",
        "# =====================================================\n",
        "print(\"\\n🧩 Chain Structure:\\n\")\n",
        "chain.get_graph().print_ascii()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64MAXZknlMSR",
        "outputId": "6905e82b-48a1-4d89-84b3-617ae6a96baf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🧩 Chain Structure:\n",
            "\n",
            "     +-------------+       \n",
            "     | PromptInput |       \n",
            "     +-------------+       \n",
            "            *              \n",
            "            *              \n",
            "            *              \n",
            "    +----------------+     \n",
            "    | PromptTemplate |     \n",
            "    +----------------+     \n",
            "            *              \n",
            "            *              \n",
            "            *              \n",
            "      +------------+       \n",
            "      | ChatOpenAI |       \n",
            "      +------------+       \n",
            "            *              \n",
            "            *              \n",
            "            *              \n",
            "   +-----------------+     \n",
            "   | StrOutputParser |     \n",
            "   +-----------------+     \n",
            "            *              \n",
            "            *              \n",
            "            *              \n",
            "+-----------------------+  \n",
            "| StrOutputParserOutput |  \n",
            "+-----------------------+  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "def log_result(topic, facts, file=\"facts_log.csv\"):\n",
        "    df = pd.DataFrame([{\n",
        "        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "        \"topic\": topic,\n",
        "        \"facts\": facts\n",
        "    }])\n",
        "    df.to_csv(file, mode=\"a\", index=False, header=not os.path.exists(file))\n",
        "    print(f\"✅ Logged results to {file}\")\n",
        "\n",
        "# Use it like:\n",
        "topic = \"cricket\"\n",
        "result = chain.invoke({\"topic\": topic})\n",
        "print(result)\n",
        "log_result(topic, result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiKgaX9QmYAr",
        "outputId": "fbd2bda0-c272-4bbd-88eb-b31c10355571"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure! Here are five interesting facts about cricket:\n",
            "\n",
            "1. **Ancient Origins**: Cricket is believed to have originated in the 16th century in England. The earliest known reference to the game dates back to 1597, and it has evolved significantly since then, becoming a popular sport worldwide.\n",
            "\n",
            "2. **Longest Match**: The longest recorded cricket match took place in 1939 between England and South Africa, lasting for 10 days! The match, held at Durban, ended in a draw after 1,199 runs were scored.\n",
            "\n",
            "3. **The Ashes**: The Ashes is one of the most celebrated rivalries in cricket, contested between England and Australia. The series began in 1882, and the term \"The Ashes\" originated from a satirical obituary published after England lost to Australia on home soil, stating that English cricket had died and \"the body will be cremated and the ashes taken to Australia.\"\n",
            "\n",
            "4. **Cricket World Cup**: The inaugural Cricket World Cup was held in 1975, and it has grown to become one of the most watched sporting events globally. The 2019 edition featured 48 matches played in England and Wales, showcasing teams from around the world competing for the prestigious title.\n",
            "\n",
            "5. **Unique Dismissals**: Cricket has several unique ways a batsman can be dismissed, some of which are quite rare. For example, a batsman can be dismissed \"hit wicket\" if they accidentally hit their own stumps with their bat or body while attempting a shot. Another interesting form of dismissal is \"obstructing the field,\" where a batsman deliberately uses their body or bat to obstruct a fielding attempt.\n",
            "✅ Logged results to facts_log.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gradio\n",
        "import gradio as gr\n",
        "\n",
        "def generate_facts(topic):\n",
        "    return chain.invoke({\"topic\": topic})\n",
        "\n",
        "gr.Interface(fn=generate_facts, inputs=\"text\", outputs=\"text\", title=\"LangChain Fact Generator\").launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "RqW2n_RvmewA",
        "outputId": "fc959a64-770a-472d-da3c-a11636c6c5f9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://3aec47d923c9d29df4.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://3aec47d923c9d29df4.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# parallel chains"
      ],
      "metadata": {
        "id": "ZQmFw9ED_WUk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "#from langchain_anthropic import ChatAnthropic\n",
        "from dotenv import load_dotenv\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableMap, RunnableSequence, RunnableParallel\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "model1 = ChatOpenAI()\n",
        "\n",
        "model2 = ChatOpenAI(model_name='gpt-4-1106-preview')\n",
        "\n",
        "prompt1 = PromptTemplate(\n",
        "    template='Generate short and simple notes from the following text \\n {text}',\n",
        "    input_variables=['text']\n",
        ")\n",
        "\n",
        "prompt2 = PromptTemplate(\n",
        "    template='Generate 5 short question answers from the following text \\n {text}',\n",
        "    input_variables=['text']\n",
        ")\n",
        "\n",
        "prompt3 = PromptTemplate(\n",
        "    template='Merge the provided notes and quiz into a single document \\n notes -> {notes} and quiz -> {quiz}',\n",
        "    input_variables=['notes', 'quiz']\n",
        ")\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "parallel_chain = RunnableParallel({\n",
        "    'notes': prompt1 | model1 | parser,\n",
        "    'quiz': prompt2 | model2 | parser\n",
        "})\n",
        "\n",
        "merge_chain = prompt3 | model1 | parser\n",
        "\n",
        "chain = parallel_chain | merge_chain\n",
        "\n",
        "text = \"\"\"\n",
        "Support vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection.\n",
        "\n",
        "The advantages of support vector machines are:\n",
        "\n",
        "Effective in high dimensional spaces.\n",
        "\n",
        "Still effective in cases where number of dimensions is greater than the number of samples.\n",
        "\n",
        "Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n",
        "\n",
        "Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\n",
        "\n",
        "The disadvantages of support vector machines include:\n",
        "\n",
        "If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.\n",
        "\n",
        "SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation (see Scores and probabilities, below).\n",
        "\n",
        "The support vector machines in scikit-learn support both dense (numpy.ndarray and convertible to that by numpy.asarray) and sparse (any scipy.sparse) sample vectors as input. However, to use an SVM to make predictions for sparse data, it must have been fit on such data. For optimal performance, use C-ordered numpy.ndarray (dense) or scipy.sparse.csr_matrix (sparse) with dtype=float64.\n",
        "\"\"\"\n",
        "\n",
        "result = chain.invoke({'text':text})\n",
        "\n",
        "print(result)\n",
        "\n",
        "chain.get_graph().print_ascii()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rb_IYmfT9mL_",
        "outputId": "b0660b11-8fb3-4c15-c4c9-6bbc0f8b0c3f"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Support Vector Machines (SVMs) are a powerful tool used for classification, regression, and outlier detection in supervised learning. They offer several advantages, such as being effective in high dimensional spaces, utilizing a subset of training points for decision making (support vectors), and being versatile with different kernel functions.\n",
            "\n",
            "However, SVMs can also have disadvantages, such as being prone to overfitting with too many features and not providing direct probability estimates. It is essential to choose the right kernel functions and regularization terms when dealing with feature-rich datasets where the number of features exceeds the number of samples to prevent overfitting.\n",
            "\n",
            "While SVMs do not provide probability estimates directly, these estimates can be calculated using a costly five-fold cross-validation process. In scikit-learn, SVMs support both dense (numpy.ndarray) and sparse (scipy.sparse) sample vectors as input. For optimal performance with dense data, it is recommended to use C-ordered numpy.ndarray with dtype=float64 and scipy.sparse.csr_matrix for sparse data.\n",
            "\n",
            "In summary, SVMs are a versatile and powerful tool in machine learning, suitable for various tasks such as classification, regression, and outlier detection. By understanding their advantages and limitations, users can leverage SVMs effectively in their data analysis projects.\n",
            "            +---------------------------+            \n",
            "            | Parallel<notes,quiz>Input |            \n",
            "            +---------------------------+            \n",
            "                 **               **                 \n",
            "              ***                   ***              \n",
            "            **                         **            \n",
            "+----------------+                +----------------+ \n",
            "| PromptTemplate |                | PromptTemplate | \n",
            "+----------------+                +----------------+ \n",
            "          *                               *          \n",
            "          *                               *          \n",
            "          *                               *          \n",
            "  +------------+                    +------------+   \n",
            "  | ChatOpenAI |                    | ChatOpenAI |   \n",
            "  +------------+                    +------------+   \n",
            "          *                               *          \n",
            "          *                               *          \n",
            "          *                               *          \n",
            "+-----------------+              +-----------------+ \n",
            "| StrOutputParser |              | StrOutputParser | \n",
            "+-----------------+              +-----------------+ \n",
            "                 **               **                 \n",
            "                   ***         ***                   \n",
            "                      **     **                      \n",
            "           +----------------------------+            \n",
            "           | Parallel<notes,quiz>Output |            \n",
            "           +----------------------------+            \n",
            "                          *                          \n",
            "                          *                          \n",
            "                          *                          \n",
            "                 +----------------+                  \n",
            "                 | PromptTemplate |                  \n",
            "                 +----------------+                  \n",
            "                          *                          \n",
            "                          *                          \n",
            "                          *                          \n",
            "                   +------------+                    \n",
            "                   | ChatOpenAI |                    \n",
            "                   +------------+                    \n",
            "                          *                          \n",
            "                          *                          \n",
            "                          *                          \n",
            "                +-----------------+                  \n",
            "                | StrOutputParser |                  \n",
            "                +-----------------+                  \n",
            "                          *                          \n",
            "                          *                          \n",
            "                          *                          \n",
            "              +-----------------------+              \n",
            "              | StrOutputParserOutput |              \n",
            "              +-----------------------+              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pydantic >=2.0"
      ],
      "metadata": {
        "id": "oTmcR9JO-VZc"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# conditional chains"
      ],
      "metadata": {
        "id": "IlODpXHL_fdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "#from langchain_anthropic import ChatAnthropic\n",
        "from dotenv import load_dotenv\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableParallel, RunnableBranch, RunnableLambda # Corrected import path\n",
        "from langchain_core.output_parsers import PydanticOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import Literal\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "model = ChatOpenAI()\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "class Feedback(BaseModel):\n",
        "    sentiment: Literal['positive', 'negative'] = Field(description='Give the sentiment of the feedback')\n",
        "\n",
        "parser2 = PydanticOutputParser(pydantic_object=Feedback)\n",
        "\n",
        "prompt1 = PromptTemplate(\n",
        "    template='Classify the sentiment of the following feedback text into postive or negative \\n {feedback} \\n {format_instruction}',\n",
        "    input_variables=['feedback'],\n",
        "    partial_variables={'format_instruction':parser2.get_format_instructions()}\n",
        ")\n",
        "\n",
        "classifier_chain = prompt1 | model | parser2\n",
        "\n",
        "prompt2 = PromptTemplate(\n",
        "    template='Write an appropriate response to this positive feedback \\n {feedback}',\n",
        "    input_variables=['feedback']\n",
        ")\n",
        "\n",
        "prompt3 = PromptTemplate(\n",
        "    template='Write an appropriate response to this negative feedback \\n {feedback}',\n",
        "    input_variables=['feedback']\n",
        ")\n",
        "\n",
        "branch_chain = RunnableBranch(\n",
        "    (lambda x:x.sentiment == 'positive', prompt2 | model | parser),\n",
        "    (lambda x:x.sentiment == 'negative', prompt3 | model | parser),\n",
        "    RunnableLambda(lambda x: \"could not find sentiment\")\n",
        ")\n",
        "\n",
        "chain = classifier_chain | branch_chain\n",
        "\n",
        "print(chain.invoke({'feedback': 'This is a beautiful phone'}))\n",
        "\n",
        "print(chain.invoke({'feedback': 'This is a terrible phone'}))\n",
        "\n",
        "print(chain.invoke({'feedback': ''}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6ZKFEWW_hvY",
        "outputId": "731a0fcc-7306-4271-fec7-c079210f5886"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thank you so much for your kind words! I'm thrilled to hear that you had a positive experience. Let me know if there's anything else I can assist you with.\n",
            "I'm sorry to hear that you had a negative experience. Can you please provide more details so that we can address your concerns and improve our service? Your feedback is important to us.\n",
            "We are sorry to hear that you had a negative experience. Please let us know how we can improve and address any issues you may have encountered. Your feedback is important to us and we value your input in helping us to better serve our customers. Thank you for sharing your concerns with us.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "chain.get_graph().print_ascii()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Md6GM8pR_5Vk",
        "outputId": "d500de42-8069-4bfd-a5f1-2035446de887"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    +-------------+      \n",
            "    | PromptInput |      \n",
            "    +-------------+      \n",
            "            *            \n",
            "            *            \n",
            "            *            \n",
            "   +----------------+    \n",
            "   | PromptTemplate |    \n",
            "   +----------------+    \n",
            "            *            \n",
            "            *            \n",
            "            *            \n",
            "     +------------+      \n",
            "     | ChatOpenAI |      \n",
            "     +------------+      \n",
            "            *            \n",
            "            *            \n",
            "            *            \n",
            "+----------------------+ \n",
            "| PydanticOutputParser | \n",
            "+----------------------+ \n",
            "            *            \n",
            "            *            \n",
            "            *            \n",
            "       +--------+        \n",
            "       | Branch |        \n",
            "       +--------+        \n",
            "            *            \n",
            "            *            \n",
            "            *            \n",
            "    +--------------+     \n",
            "    | BranchOutput |     \n",
            "    +--------------+     \n"
          ]
        }
      ]
    }
  ]
}